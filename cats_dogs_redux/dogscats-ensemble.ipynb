{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "from theano.sandbox import cuda\n",
    "cuda.use('gpu0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "homeDir = os.getcwd()\n",
    "dataDir = homeDir + \"/data/\"\n",
    "train_path = dataDir + \"train/\"\n",
    "valid_path = dataDir + \"valid/\"\n",
    "model_path = homeDir + \"/data/models/\"\n",
    "test_path = dataDir + \"test\"\n",
    "results_path = dataDir + \"/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = dataDir\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(path+'train', shuffle=False, batch_size=batch_size)\n",
    "val_batches = get_batches(path+'valid', shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, \n",
    "    val_filenames, filenames, test_filenames) = get_classes(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we're going to create an ensemble of models and use their average as our predictions. For each ensemble, we're going to follow our usual fine-tuning steps:\n",
    "\n",
    "1) Create a model that retrains just the last layer\n",
    "2) Add this to a model containing all VGG layers except the last layer\n",
    "3) Fine-tune just the dense layers of this model (pre-computing the convolutional layers)\n",
    "4) Add data augmentation, fine-tuning the dense layers without pre-computation.\n",
    "\n",
    "So first, we need to create our VGG model and pre-compute the output of the conv layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Vgg16 import Vgg16\n",
    "from vgg16bn import Vgg16BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Vgg16BN().model\n",
    "conv_layers,fc_layers = split_at(model, Convolution2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_model = Sequential(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_features = conv_model.predict_generator(val_batches, val_batches.nb_sample)\n",
    "trn_features = conv_model.predict_generator(batches, batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(model_path + 'train_convlayer_features.bc', trn_features)\n",
    "save_array(model_path + 'valid_convlayer_features.bc', val_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future we can just load these precomputed features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_features = load_array(model_path+'train_convlayer_features.bc')\n",
    "val_features = load_array(model_path+'valid_convlayer_features.bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save some time by pre-computing the training and validation arrays with the image decoding and resizing already done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trn = get_data(path+'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val = get_data(path+'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_array(model_path+'train_data.bc', trn)\n",
    "save_array(model_path+'valid_data.bc', val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future we can just load these resized images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn = load_array(model_path+'train_data.bc')\n",
    "val = load_array(model_path+'valid_data.bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can precompute the output of all but the last dropout and dense layers, for creating the first stage of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.pop()\n",
    "model.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ll_val_feat = model.predict_generator(val_batches, val_batches.nb_sample)\n",
    "ll_feat = model.predict_generator(batches, batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(model_path + 'train_ll_feat.bc', ll_feat)\n",
    "save_array(model_path + 'valid_ll_feat.bc', ll_val_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ll_feat = load_array(model_path+ 'train_ll_feat.bc')\n",
    "ll_val_feat = load_array(model_path + 'valid_ll_feat.bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and let's also grab the test data, for when we need to submit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = get_data(path+'test')\n",
    "save_array(model_path+'test_data.bc', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = load_array(model_path+'test_data.bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions automate creating a model that trains the last layer from scratch, and then adds those new layers on to the main model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_ll_layers():\n",
    "    return [ \n",
    "        BatchNormalization(input_shape=(4096,)),\n",
    "        Dropout(0.5),\n",
    "        Dense(2, activation='softmax') \n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_last_layer(i):\n",
    "    ll_layers = get_ll_layers()\n",
    "    ll_model = Sequential(ll_layers)\n",
    "    ll_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    ll_model.optimizer.lr=1e-5\n",
    "    ll_model.fit(ll_feat, trn_labels, validation_data=(ll_val_feat, val_labels), nb_epoch=12)\n",
    "    ll_model.optimizer.lr=1e-7\n",
    "    ll_model.fit(ll_feat, trn_labels, validation_data=(ll_val_feat, val_labels), nb_epoch=1)\n",
    "    ll_model.save_weights(model_path+'ll_bn' + i + '.h5')\n",
    "\n",
    "    vgg = Vgg16BN()\n",
    "    model = vgg.model\n",
    "    model.pop(); model.pop(); model.pop()\n",
    "    for layer in model.layers: layer.trainable=False\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    ll_layers = get_ll_layers()\n",
    "    for layer in ll_layers: model.add(layer)\n",
    "    for l1,l2 in zip(ll_model.layers, model.layers[-3:]):\n",
    "        l2.set_weights(l1.get_weights())\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.save_weights(model_path+'bn' + i + '.h5')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_conv_model(model):\n",
    "    layers = model.layers\n",
    "    last_conv_idx = [index for index,layer in enumerate(layers) \n",
    "                         if type(layer) is Convolution2D][-1]\n",
    "\n",
    "    conv_layers = layers[:last_conv_idx+1]\n",
    "    conv_model = Sequential(conv_layers)\n",
    "    fc_layers = layers[last_conv_idx+1:]\n",
    "    return conv_model, fc_layers, last_conv_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fc_layers(p, in_shape):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=in_shape),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(4096, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(2, activation='softmax')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_dense_layers(i, model):\n",
    "    conv_model, fc_layers, last_conv_idx = get_conv_model(model)\n",
    "    conv_shape = conv_model.output_shape[1:]\n",
    "    fc_model = Sequential(get_fc_layers(0.5, conv_shape))\n",
    "    for l1,l2 in zip(fc_model.layers, fc_layers): \n",
    "        weights = l2.get_weights()\n",
    "        l1.set_weights(weights)\n",
    "    fc_model.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy', \n",
    "                     metrics=['accuracy'])\n",
    "    fc_model.fit(trn_features, trn_labels, nb_epoch=2, \n",
    "         batch_size=batch_size, validation_data=(val_features, val_labels))\n",
    "\n",
    "    gen = image.ImageDataGenerator(rotation_range=10, width_shift_range=0.05, zoom_range=0.05,\n",
    "       channel_shift_range=10, height_shift_range=0.05, shear_range=0.05, horizontal_flip=True)\n",
    "    batches = gen.flow(trn, trn_labels, batch_size=batch_size)\n",
    "    val_batches = image.ImageDataGenerator().flow(val, val_labels, \n",
    "                      shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    for layer in conv_model.layers: layer.trainable = False\n",
    "    for layer in get_fc_layers(0.5, conv_shape): conv_model.add(layer)\n",
    "    for l1,l2 in zip(conv_model.layers[last_conv_idx+1:], fc_model.layers): \n",
    "        l1.set_weights(l2.get_weights())\n",
    "\n",
    "    conv_model.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy', \n",
    "                       metrics=['accuracy'])\n",
    "    #conv_model.save_weights(model_path+'no_dropout_bn' + i + '.h5')\n",
    "    conv_model.fit_generator(batches, samples_per_epoch=batches.N, nb_epoch=1,\n",
    "                             validation_data=val_batches, nb_val_samples=val_batches.N)\n",
    "    conv_model.save_weights(model_path+'m1' + i + '.h5')\n",
    "    for layer in conv_model.layers[16:]: layer.trainable = True\n",
    "    conv_model.fit_generator(batches, samples_per_epoch=batches.N, nb_epoch=1, \n",
    "                            validation_data=val_batches, nb_val_samples=val_batches.N)\n",
    "    conv_model.save_weights(model_path+'m2' + i + '.h5')\n",
    "\n",
    "    #conv_model.optimizer.lr = 1e-7\n",
    "    #conv_model.fit_generator(batches, samples_per_epoch=batches.N, nb_epoch=3, \n",
    "                           # validation_data=val_batches, nb_val_samples=val_batches.N)\n",
    "    #conv_model.save_weights(model_path + 'aug' + i + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Build ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.5398 - acc: 0.8076 - val_loss: 2.3753 - val_acc: 0.3735\n",
      "Epoch 2/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4618 - acc: 0.8607 - val_loss: 2.5498 - val_acc: 0.3760\n",
      "Epoch 3/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4490 - acc: 0.8702 - val_loss: 2.4868 - val_acc: 0.3730\n",
      "Epoch 4/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4355 - acc: 0.8740 - val_loss: 2.5565 - val_acc: 0.3735\n",
      "Epoch 5/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4260 - acc: 0.8762 - val_loss: 2.5125 - val_acc: 0.3715\n",
      "Epoch 6/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4213 - acc: 0.8776 - val_loss: 2.4618 - val_acc: 0.3720\n",
      "Epoch 7/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4178 - acc: 0.8787 - val_loss: 2.5337 - val_acc: 0.3695\n",
      "Epoch 8/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4080 - acc: 0.8816 - val_loss: 2.5892 - val_acc: 0.3690\n",
      "Epoch 9/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.3978 - acc: 0.8848 - val_loss: 2.4920 - val_acc: 0.3695\n",
      "Epoch 10/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.3947 - acc: 0.8840 - val_loss: 2.4956 - val_acc: 0.3690\n",
      "Epoch 11/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.3895 - acc: 0.8842 - val_loss: 2.5044 - val_acc: 0.3685\n",
      "Epoch 12/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.3815 - acc: 0.8870 - val_loss: 2.3134 - val_acc: 0.3685\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 3s - loss: 0.3770 - acc: 0.8895 - val_loss: 2.3974 - val_acc: 0.3685\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/2\n",
      "23000/23000 [==============================] - 42s - loss: 0.0851 - acc: 0.9700 - val_loss: 0.0508 - val_acc: 0.9815\n",
      "Epoch 2/2\n",
      "23000/23000 [==============================] - 42s - loss: 0.0346 - acc: 0.9873 - val_loss: 0.0551 - val_acc: 0.9815\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 656s - loss: 0.0544 - acc: 0.9803 - val_loss: 0.0590 - val_acc: 0.9815\n",
      "Epoch 1/3\n",
      "23000/23000 [==============================] - 656s - loss: 0.0447 - acc: 0.9833 - val_loss: 0.0603 - val_acc: 0.9795\n",
      "Epoch 2/3\n",
      "23000/23000 [==============================] - 656s - loss: 0.0338 - acc: 0.9875 - val_loss: 0.0630 - val_acc: 0.9820\n",
      "Epoch 3/3\n",
      "23000/23000 [==============================] - 655s - loss: 0.0317 - acc: 0.9882 - val_loss: 0.0593 - val_acc: 0.9830\n",
      "Epoch 1/3\n",
      "23000/23000 [==============================] - 655s - loss: 0.0249 - acc: 0.9911 - val_loss: 0.0598 - val_acc: 0.9820\n",
      "Epoch 2/3\n",
      "23000/23000 [==============================] - 655s - loss: 0.0268 - acc: 0.9907 - val_loss: 0.0710 - val_acc: 0.9805\n",
      "Epoch 3/3\n",
      "23000/23000 [==============================] - 655s - loss: 0.0227 - acc: 0.9922 - val_loss: 0.0649 - val_acc: 0.9820\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.7004 - acc: 0.7345 - val_loss: 2.0662 - val_acc: 0.3780\n",
      "Epoch 2/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4760 - acc: 0.8520 - val_loss: 2.4022 - val_acc: 0.3745\n",
      "Epoch 3/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4463 - acc: 0.8697 - val_loss: 2.5480 - val_acc: 0.3715\n",
      "Epoch 4/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4300 - acc: 0.8734 - val_loss: 2.4756 - val_acc: 0.3685\n",
      "Epoch 5/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4186 - acc: 0.8777 - val_loss: 2.4814 - val_acc: 0.3675\n",
      "Epoch 6/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4147 - acc: 0.8791 - val_loss: 2.5344 - val_acc: 0.3685\n",
      "Epoch 7/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4058 - acc: 0.8826 - val_loss: 2.5378 - val_acc: 0.3685\n",
      "Epoch 8/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.3997 - acc: 0.8846 - val_loss: 2.4092 - val_acc: 0.3695\n",
      "Epoch 9/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.3932 - acc: 0.8849 - val_loss: 2.5078 - val_acc: 0.3695\n",
      "Epoch 10/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.3891 - acc: 0.8867 - val_loss: 2.3827 - val_acc: 0.3690\n",
      "Epoch 11/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.3854 - acc: 0.8864 - val_loss: 2.4472 - val_acc: 0.3670\n",
      "Epoch 12/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.3757 - acc: 0.8862 - val_loss: 2.4252 - val_acc: 0.3670\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 3s - loss: 0.3739 - acc: 0.8884 - val_loss: 2.2632 - val_acc: 0.3670\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/2\n",
      "23000/23000 [==============================] - 41s - loss: 0.0862 - acc: 0.9696 - val_loss: 0.0567 - val_acc: 0.9790\n",
      "Epoch 2/2\n",
      "23000/23000 [==============================] - 41s - loss: 0.0392 - acc: 0.9858 - val_loss: 0.0625 - val_acc: 0.9830\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 652s - loss: 0.0498 - acc: 0.9827 - val_loss: 0.0561 - val_acc: 0.9830\n",
      "Epoch 1/3\n",
      "23000/23000 [==============================] - 652s - loss: 0.0384 - acc: 0.9862 - val_loss: 0.0595 - val_acc: 0.9820\n",
      "Epoch 2/3\n",
      "23000/23000 [==============================] - 651s - loss: 0.0236 - acc: 0.9915 - val_loss: 0.0603 - val_acc: 0.9815\n",
      "Epoch 3/3\n",
      "23000/23000 [==============================] - 651s - loss: 0.0196 - acc: 0.9930 - val_loss: 0.0617 - val_acc: 0.9820\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.6079 - acc: 0.7787 - val_loss: 2.3478 - val_acc: 0.3675\n",
      "Epoch 2/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4859 - acc: 0.8544 - val_loss: 2.6880 - val_acc: 0.3660\n",
      "Epoch 3/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4455 - acc: 0.8675 - val_loss: 2.6652 - val_acc: 0.3640\n",
      "Epoch 4/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4387 - acc: 0.8728 - val_loss: 2.5909 - val_acc: 0.3640\n",
      "Epoch 5/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4308 - acc: 0.8735 - val_loss: 2.6396 - val_acc: 0.3665\n",
      "Epoch 6/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4230 - acc: 0.8789 - val_loss: 2.5295 - val_acc: 0.3675\n",
      "Epoch 7/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4095 - acc: 0.8805 - val_loss: 2.5772 - val_acc: 0.3660\n",
      "Epoch 8/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4042 - acc: 0.8847 - val_loss: 2.5063 - val_acc: 0.3650\n",
      "Epoch 9/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.3964 - acc: 0.8871 - val_loss: 2.5374 - val_acc: 0.3650\n",
      "Epoch 10/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4003 - acc: 0.8818 - val_loss: 2.4720 - val_acc: 0.3660\n",
      "Epoch 11/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.3911 - acc: 0.8855 - val_loss: 2.4407 - val_acc: 0.3660\n",
      "Epoch 12/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.3873 - acc: 0.8875 - val_loss: 2.4417 - val_acc: 0.3670\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 3s - loss: 0.3822 - acc: 0.8887 - val_loss: 2.4928 - val_acc: 0.3670\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/2\n",
      "23000/23000 [==============================] - 41s - loss: 0.0834 - acc: 0.9691 - val_loss: 0.0581 - val_acc: 0.9780\n",
      "Epoch 2/2\n",
      "23000/23000 [==============================] - 41s - loss: 0.0366 - acc: 0.9877 - val_loss: 0.0557 - val_acc: 0.9800\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 652s - loss: 0.0524 - acc: 0.9807 - val_loss: 0.0576 - val_acc: 0.9820\n",
      "Epoch 1/3\n",
      "23000/23000 [==============================] - 903s - loss: 0.0454 - acc: 0.9843 - val_loss: 0.0544 - val_acc: 0.9835\n",
      "Epoch 2/3\n",
      "23000/23000 [==============================] - 651s - loss: 0.0370 - acc: 0.9871 - val_loss: 0.0559 - val_acc: 0.9830\n",
      "Epoch 3/3\n",
      "23000/23000 [==============================] - 651s - loss: 0.0295 - acc: 0.9891 - val_loss: 0.0599 - val_acc: 0.9820\n",
      "Epoch 1/3\n",
      "23000/23000 [==============================] - 784s - loss: 0.0276 - acc: 0.9897 - val_loss: 0.0604 - val_acc: 0.9825\n",
      "Epoch 2/3\n",
      "23000/23000 [==============================] - 770s - loss: 0.0230 - acc: 0.9914 - val_loss: 0.0614 - val_acc: 0.9825\n",
      "Epoch 3/3\n",
      "23000/23000 [==============================] - 903s - loss: 0.0215 - acc: 0.9922 - val_loss: 0.0704 - val_acc: 0.9830\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.5831 - acc: 0.7895 - val_loss: 2.3358 - val_acc: 0.3795\n",
      "Epoch 2/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4710 - acc: 0.8548 - val_loss: 2.4336 - val_acc: 0.3735\n",
      "Epoch 3/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4483 - acc: 0.8670 - val_loss: 2.5465 - val_acc: 0.3735\n",
      "Epoch 4/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4347 - acc: 0.8762 - val_loss: 2.4794 - val_acc: 0.3695\n",
      "Epoch 5/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4133 - acc: 0.8797 - val_loss: 2.5442 - val_acc: 0.3710\n",
      "Epoch 6/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4197 - acc: 0.8759 - val_loss: 2.5576 - val_acc: 0.3710\n",
      "Epoch 7/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4025 - acc: 0.8832 - val_loss: 2.4941 - val_acc: 0.3690\n",
      "Epoch 8/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4082 - acc: 0.8812 - val_loss: 2.5452 - val_acc: 0.3670\n",
      "Epoch 9/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.3969 - acc: 0.8821 - val_loss: 2.4673 - val_acc: 0.3675\n",
      "Epoch 10/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.3979 - acc: 0.8841 - val_loss: 2.4948 - val_acc: 0.3680\n",
      "Epoch 11/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.3873 - acc: 0.8861 - val_loss: 2.4086 - val_acc: 0.3670\n",
      "Epoch 12/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.3832 - acc: 0.8891 - val_loss: 2.3718 - val_acc: 0.3670\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 3s - loss: 0.3867 - acc: 0.8867 - val_loss: 2.4262 - val_acc: 0.3665\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/2\n",
      "23000/23000 [==============================] - 41s - loss: 0.0850 - acc: 0.9698 - val_loss: 0.0581 - val_acc: 0.9780\n",
      "Epoch 2/2\n",
      "23000/23000 [==============================] - 59s - loss: 0.0342 - acc: 0.9877 - val_loss: 0.0588 - val_acc: 0.9800\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 870s - loss: 0.0576 - acc: 0.9798 - val_loss: 0.0634 - val_acc: 0.9820\n",
      "Epoch 1/3\n",
      "23000/23000 [==============================] - 652s - loss: 0.0393 - acc: 0.9854 - val_loss: 0.0574 - val_acc: 0.9810\n",
      "Epoch 2/3\n",
      "23000/23000 [==============================] - 651s - loss: 0.0367 - acc: 0.9870 - val_loss: 0.0590 - val_acc: 0.9790\n",
      "Epoch 3/3\n",
      "23000/23000 [==============================] - 651s - loss: 0.0260 - acc: 0.9906 - val_loss: 0.0582 - val_acc: 0.9830\n",
      "Epoch 1/3\n",
      "23000/23000 [==============================] - 652s - loss: 0.0259 - acc: 0.9913 - val_loss: 0.0599 - val_acc: 0.9835\n",
      "Epoch 2/3\n",
      "23000/23000 [==============================] - 651s - loss: 0.0237 - acc: 0.9913 - val_loss: 0.0704 - val_acc: 0.9805\n",
      "Epoch 3/3\n",
      "23000/23000 [==============================] - 651s - loss: 0.0201 - acc: 0.9932 - val_loss: 0.0683 - val_acc: 0.9820\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.6882 - acc: 0.7394 - val_loss: 2.1775 - val_acc: 0.3790\n",
      "Epoch 2/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4864 - acc: 0.8507 - val_loss: 2.5139 - val_acc: 0.3735\n",
      "Epoch 3/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4558 - acc: 0.8655 - val_loss: 2.5045 - val_acc: 0.3720\n",
      "Epoch 4/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4408 - acc: 0.8721 - val_loss: 2.5187 - val_acc: 0.3725\n",
      "Epoch 5/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4426 - acc: 0.8734 - val_loss: 2.5918 - val_acc: 0.3710\n",
      "Epoch 6/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4248 - acc: 0.8759 - val_loss: 2.5279 - val_acc: 0.3690\n",
      "Epoch 7/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4192 - acc: 0.8792 - val_loss: 2.4405 - val_acc: 0.3705\n",
      "Epoch 8/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4071 - acc: 0.8825 - val_loss: 2.5326 - val_acc: 0.3705\n",
      "Epoch 9/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4053 - acc: 0.8823 - val_loss: 2.5658 - val_acc: 0.3680\n",
      "Epoch 10/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4013 - acc: 0.8820 - val_loss: 2.4827 - val_acc: 0.3700\n",
      "Epoch 11/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4035 - acc: 0.8819 - val_loss: 2.4321 - val_acc: 0.3710\n",
      "Epoch 12/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.3843 - acc: 0.8877 - val_loss: 2.3864 - val_acc: 0.3690\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 3s - loss: 0.3818 - acc: 0.8859 - val_loss: 2.4166 - val_acc: 0.3685\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/2\n",
      "23000/23000 [==============================] - 41s - loss: 0.0844 - acc: 0.9691 - val_loss: 0.0537 - val_acc: 0.9815\n",
      "Epoch 2/2\n",
      "23000/23000 [==============================] - 41s - loss: 0.0374 - acc: 0.9866 - val_loss: 0.0587 - val_acc: 0.9820\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 652s - loss: 0.0471 - acc: 0.9838 - val_loss: 0.0573 - val_acc: 0.9835\n",
      "Epoch 1/3\n",
      "23000/23000 [==============================] - 652s - loss: 0.0465 - acc: 0.9841 - val_loss: 0.0656 - val_acc: 0.9820\n",
      "Epoch 2/3\n",
      "23000/23000 [==============================] - 652s - loss: 0.0399 - acc: 0.9868 - val_loss: 0.0656 - val_acc: 0.9820\n",
      "Epoch 3/3\n",
      "12224/23000 [==============>...............] - ETA: 282s - loss: 0.0320 - acc: 0.9887"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    i = str(i)\n",
    "    model = train_last_layer(i)\n",
    "    train_dense_layers(i, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/12\n",
      "23000/23000 [==============================] - 4s - loss: 0.6407 - acc: 0.7653 - val_loss: 2.2491 - val_acc: 0.3835\n",
      "Epoch 2/12\n",
      "23000/23000 [==============================] - 4s - loss: 0.4806 - acc: 0.8523 - val_loss: 2.5045 - val_acc: 0.3750\n",
      "Epoch 3/12\n",
      "23000/23000 [==============================] - 4s - loss: 0.4605 - acc: 0.8632 - val_loss: 2.5822 - val_acc: 0.3735\n",
      "Epoch 4/12\n",
      "23000/23000 [==============================] - 4s - loss: 0.4363 - acc: 0.8710 - val_loss: 2.4512 - val_acc: 0.3720\n",
      "Epoch 5/12\n",
      "23000/23000 [==============================] - 4s - loss: 0.4262 - acc: 0.8757 - val_loss: 2.4646 - val_acc: 0.3715\n",
      "Epoch 6/12\n",
      "23000/23000 [==============================] - 4s - loss: 0.4127 - acc: 0.8785 - val_loss: 2.4646 - val_acc: 0.3710\n",
      "Epoch 7/12\n",
      "23000/23000 [==============================] - 4s - loss: 0.4142 - acc: 0.8790 - val_loss: 2.4168 - val_acc: 0.3705\n",
      "Epoch 8/12\n",
      "23000/23000 [==============================] - 4s - loss: 0.3988 - acc: 0.8822 - val_loss: 2.4477 - val_acc: 0.3710\n",
      "Epoch 9/12\n",
      "23000/23000 [==============================] - 5s - loss: 0.4033 - acc: 0.8828 - val_loss: 2.5005 - val_acc: 0.3715\n",
      "Epoch 10/12\n",
      "23000/23000 [==============================] - 5s - loss: 0.4010 - acc: 0.8824 - val_loss: 2.3994 - val_acc: 0.3690\n",
      "Epoch 11/12\n",
      "23000/23000 [==============================] - 4s - loss: 0.3856 - acc: 0.8850 - val_loss: 2.4710 - val_acc: 0.3685\n",
      "Epoch 12/12\n",
      "23000/23000 [==============================] - 4s - loss: 0.3817 - acc: 0.8852 - val_loss: 2.3657 - val_acc: 0.3690\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 5s - loss: 0.3765 - acc: 0.8897 - val_loss: 2.3987 - val_acc: 0.3700\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/2\n",
      "23000/23000 [==============================] - 44s - loss: 0.0816 - acc: 0.9700 - val_loss: 0.0618 - val_acc: 0.9795\n",
      "Epoch 2/2\n",
      "23000/23000 [==============================] - 45s - loss: 0.0416 - acc: 0.9858 - val_loss: 0.0633 - val_acc: 0.9765\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 654s - loss: 0.0541 - acc: 0.9817 - val_loss: 0.0663 - val_acc: 0.9785\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 654s - loss: 0.0422 - acc: 0.9854 - val_loss: 0.0699 - val_acc: 0.9785\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.6259 - acc: 0.7693 - val_loss: 2.2327 - val_acc: 0.3805\n",
      "Epoch 2/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4817 - acc: 0.8522 - val_loss: 2.4662 - val_acc: 0.3745\n",
      "Epoch 3/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4479 - acc: 0.8679 - val_loss: 2.6679 - val_acc: 0.3735\n",
      "Epoch 4/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4469 - acc: 0.8707 - val_loss: 2.6485 - val_acc: 0.3730\n",
      "Epoch 5/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4299 - acc: 0.8764 - val_loss: 2.5573 - val_acc: 0.3715\n",
      "Epoch 6/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4143 - acc: 0.8810 - val_loss: 2.5310 - val_acc: 0.3715\n",
      "Epoch 7/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4086 - acc: 0.8795 - val_loss: 2.5105 - val_acc: 0.3705\n",
      "Epoch 8/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4044 - acc: 0.8809 - val_loss: 2.4629 - val_acc: 0.3705\n",
      "Epoch 9/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4027 - acc: 0.8822 - val_loss: 2.4779 - val_acc: 0.3695\n",
      "Epoch 10/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.3978 - acc: 0.8829 - val_loss: 2.4494 - val_acc: 0.3685\n",
      "Epoch 11/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.3874 - acc: 0.8849 - val_loss: 2.3444 - val_acc: 0.3680\n",
      "Epoch 12/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.3926 - acc: 0.8867 - val_loss: 2.3903 - val_acc: 0.3670\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 3s - loss: 0.3866 - acc: 0.8837 - val_loss: 2.5182 - val_acc: 0.3675\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/2\n",
      "23000/23000 [==============================] - 41s - loss: 0.0866 - acc: 0.9682 - val_loss: 0.0552 - val_acc: 0.9795\n",
      "Epoch 2/2\n",
      "23000/23000 [==============================] - 41s - loss: 0.0373 - acc: 0.9865 - val_loss: 0.0558 - val_acc: 0.9795\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 652s - loss: 0.0551 - acc: 0.9814 - val_loss: 0.0621 - val_acc: 0.9810\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 652s - loss: 0.0434 - acc: 0.9847 - val_loss: 0.0647 - val_acc: 0.9825\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    i = str(i)\n",
    "    model = train_last_layer(i)\n",
    "    train_dense_layers(i, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine ensemble and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ens_model = vgg_ft(2)\n",
    "for layer in ens_model.layers: layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg = Vgg16BN()\n",
    "model = vgg.model\n",
    "model.pop()\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "for layer in model.layers: layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_ens_pred(arr, fname):\n",
    "    ens_pred = []\n",
    "    for i in [0,1,3,4]:\n",
    "        i = str(i)\n",
    "        model.load_weights('{}{}{}.h5'.format(model_path, fname, i))\n",
    "        preds = model.predict(arr, batch_size=64)\n",
    "        ens_pred.append(preds)\n",
    "    return ens_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_comb_pred(arr):\n",
    "    ens_pred = []\n",
    "    for i in ['m10','m11','m20','m21']:\n",
    "        model.load_weights('{}{}.h5'.format(model_path, i))\n",
    "        preds = model.predict(arr, batch_size=64)\n",
    "        ens_pred.append(preds)\n",
    "    return ens_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-d2c0ffa2f6dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mval_pred2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ens_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'aug'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'val' is not defined"
     ]
    }
   ],
   "source": [
    "val_pred2 = get_ens_pred(val, 'aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_avg_preds2 = np.stack(val_pred2).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categorical_accuracy(val_labels, val_avg_preds2).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = get_ens_pred(test, 'aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_avg_preds = np.stack(test_pred).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/KaggleCompetitions/cats_dogs_redux/data\n",
      "/home/ubuntu/KaggleCompetitions/cats_dogs_redux\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='data/submission_ensemble3.csv' target='_blank'>data/submission_ensemble3.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/KaggleCompetitions/cats_dogs_redux/data/submission_ensemble3.csv"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = test_filenames\n",
    "isdog = test_avg_preds[:,1]\n",
    "isdog = isdog.clip(min=0.005, max=0.995)\n",
    "ids = np.array([int(f[8:f.find('.')]) for f in filenames])\n",
    "subm = np.stack([ids,isdog], axis=1)\n",
    "subm[:5]\n",
    "%cd $dataDir\n",
    "submission_file_name = 'submission_ensemble3.csv'\n",
    "np.savetxt(submission_file_name, subm, fmt='%d,%.5f', header='id,label', comments='')\n",
    "from IPython.display import FileLink\n",
    "%cd ../\n",
    "FileLink('data/'+submission_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
