{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import os, sys, glob\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from shutil import copyfile\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "homeDir = os.getcwd()\n",
    "dataHome = homeDir + \"/data/\"\n",
    "dataDir = homeDir + \"/data/\"\n",
    "#dataDir = homeDir + \"/data/sample/\"\n",
    "train_path = dataDir + \"train/\"\n",
    "valid_path = dataDir + \"valid/\"\n",
    "model_path = dataDir + \"models/\"\n",
    "test_path = dataHome + \"test/\"\n",
    "results_path = dataDir + \"results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/KaggleCompetitions/Fisheries\n"
     ]
    }
   ],
   "source": [
    "%cd $homeDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from Vgg16 import Vgg16\n",
    "from vgg16bn import Vgg16BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg = Vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set constants. You can experiment with no_of_epochs to improve the model. You can reduce the batch_size \n",
    "#depending on the memory contraints of gpu\n",
    "batch_size=64\n",
    "no_of_epochs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3342 images belonging to 8 classes.\n",
      "Found 435 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = vgg.get_batches(train_path, batch_size=batch_size)\n",
    "val_batches = vgg.get_batches(valid_path, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.model.pop() #Remove the final layer\n",
    "for layer in vgg.model.layers:\n",
    "    layer.trainable=False # Set all other layers to untrainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.model.add(Dense(8, activation='softmax')) \n",
    "#Adding a new dense layer wiht only 8 outputs and softmax acitvation as it is the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.model.load_weights(results_path+'f1LastLayer.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.compile(lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "3342/3342 [==============================] - 101s - loss: 1.0587 - acc: 0.6795 - val_loss: 0.5772 - val_acc: 0.7977\n"
     ]
    }
   ],
   "source": [
    "vgg.fit(batches, val_batches, nb_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3342/3342 [==============================] - 99s - loss: 1.0242 - acc: 0.6900 - val_loss: 0.5403 - val_acc: 0.8092\n",
      "Epoch 2/2\n",
      "3342/3342 [==============================] - 94s - loss: 0.9594 - acc: 0.6954 - val_loss: 0.5455 - val_acc: 0.8046\n"
     ]
    }
   ],
   "source": [
    "vgg.fit(batches, val_batches, nb_epoch=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.compile(lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3342/3342 [==============================] - 98s - loss: 1.0162 - acc: 0.6879 - val_loss: 0.4696 - val_acc: 0.8207\n",
      "Epoch 2/2\n",
      "3342/3342 [==============================] - 93s - loss: 0.9284 - acc: 0.7223 - val_loss: 0.5598 - val_acc: 0.8115\n"
     ]
    }
   ],
   "source": [
    "vgg.fit(batches, val_batches, nb_epoch=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3342/3342 [==============================] - 99s - loss: 0.7715 - acc: 0.7636 - val_loss: 0.3728 - val_acc: 0.8690\n",
      "Epoch 2/2\n",
      "3342/3342 [==============================] - 94s - loss: 0.6942 - acc: 0.7573 - val_loss: 0.4105 - val_acc: 0.8598\n"
     ]
    }
   ],
   "source": [
    "vgg.compile(lr = 0.0001)\n",
    "vgg.fit(batches, val_batches, nb_epoch=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "tbatches, preds = vgg.test(test_path, batch_size = 64)\n",
    "filenames = tbatches.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = np.clip(preds,0.01,0.99)\n",
    "files = [s.split('/')[-1] for s in filenames]\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(preds, columns=['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT'])\n",
    "df.insert(0,'image', files)\n",
    "subFile = results_path + 'finetuningthebeforebestmodel3clip.csv'\n",
    "df.to_csv(subFile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
